{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T15:47:58.861208Z",
     "iopub.status.busy": "2025-12-20T15:47:58.860562Z",
     "iopub.status.idle": "2025-12-20T15:47:58.866065Z",
     "shell.execute_reply": "2025-12-20T15:47:58.865334Z",
     "shell.execute_reply.started": "2025-12-20T15:47:58.861183Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Voice Packet Reconstruction - GAN VERSION\n",
    "Sharp, realistic reconstruction using GAN + L1 loss\n",
    "Architecture:\n",
    "- Generator: Transformer (existing model)\n",
    "- Discriminator: PatchGAN (judges local patches for realism)\n",
    "- Loss: L1 + Adversarial + Feature Matching\n",
    "\n",
    "This fixes MSE blurring and produces sharp, realistic spectrograms!\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import urllib.request\n",
    "from IPython.display import Audio\n",
    "import tarfile\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T15:48:38.560417Z",
     "iopub.status.busy": "2025-12-20T15:48:38.559641Z",
     "iopub.status.idle": "2025-12-20T15:48:38.565002Z",
     "shell.execute_reply": "2025-12-20T15:48:38.564156Z",
     "shell.execute_reply.started": "2025-12-20T15:48:38.560391Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'data_dir': '/kaggle/working/audio_data',\n",
    "    'ljspeech_url': 'https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2',\n",
    "    'checkpoint_dir': '/kaggle/working/checkpoints',\n",
    "    'output_dir': '/kaggle/working/outputs',\n",
    "    'input_model_path': '/kaggle/input/holomodel/pytorch/default/3/my_folder/best_model_gan.pt',\n",
    "    'batch_size': 16,\n",
    "    'num_epochs': 10,\n",
    "    'learning_rate_g': 2e-4,\n",
    "    'learning_rate_d': 5e-5,  # MUCH slower discriminator learning\n",
    "    'packet_loss_min': 0.1,\n",
    "    'packet_loss_max': 0.5,\n",
    "    'num_files_to_use': 1000,\n",
    "    'weight_decay': 0.01,\n",
    "    'early_stopping_patience': 10,\n",
    "    'lambda_l1': 100.0,\n",
    "    'lambda_adv': 0.1,    # REDUCED from 1.0 - discriminator too strong\n",
    "    'lambda_fm': 10.0,\n",
    "    # Label smoothing\n",
    "    'real_label': 0.9,    # Smooth real labels (instead of 1.0)\n",
    "    'fake_label': 0.0,    # Keep fake labels at 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T15:48:40.066507Z",
     "iopub.status.busy": "2025-12-20T15:48:40.065843Z",
     "iopub.status.idle": "2025-12-20T15:48:40.097664Z",
     "shell.execute_reply": "2025-12-20T15:48:40.096976Z",
     "shell.execute_reply.started": "2025-12-20T15:48:40.066483Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Generator\n",
    "class PacketLossReconstructor(nn.Module):\n",
    "    \"\"\"\n",
    "    Generator: Transformer-based reconstruction model.\n",
    "    Same as before i.e., U-Net style with skip connections.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_mels=128, hidden_dim=512, num_layers=6, dropout=0.3, max_len=500):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_mels = n_mels\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        self.pos_encoding = nn.Parameter(torch.randn(1, max_len, n_mels) * 0.02)\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=n_mels,\n",
    "                nhead=8,\n",
    "                dim_feedforward=hidden_dim,\n",
    "                dropout=dropout,\n",
    "                batch_first=True,\n",
    "                activation='gelu'\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Decoder with skip connections\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=n_mels * 2,\n",
    "                nhead=8,\n",
    "                dim_feedforward=hidden_dim,\n",
    "                dropout=dropout,\n",
    "                batch_first=True,\n",
    "                activation='gelu'\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.skip_projections = nn.ModuleList([\n",
    "            nn.Linear(n_mels * 2, n_mels) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(n_mels, n_mels * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(n_mels * 2, n_mels)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        seq_len = x.size(1)\n",
    "        \n",
    "        if seq_len > self.max_len:\n",
    "            x = x[:, :self.max_len, :]\n",
    "            if mask is not None:\n",
    "                mask = mask[:, :self.max_len]\n",
    "            seq_len = self.max_len\n",
    "        \n",
    "        x = x + self.pos_encoding[:, :seq_len, :]\n",
    "        \n",
    "        # Encode with skip connections\n",
    "        skip_connections = []\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            x = encoder_layer(x)\n",
    "            skip_connections.append(x)\n",
    "        \n",
    "        # Decode with skip connections\n",
    "        for decoder_layer, skip_proj, skip in zip(\n",
    "            self.decoder_layers, self.skip_projections, reversed(skip_connections)\n",
    "        ):\n",
    "            x = torch.cat([x, skip], dim=-1)\n",
    "            x = decoder_layer(x)\n",
    "            x = skip_proj(x)\n",
    "        \n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Discriminator\n",
    "class PatchGANDiscriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    PatchGAN discriminator for spectrograms.\n",
    "    Judges whether local patches are real or fake.\n",
    "    Also returns intermediate features for feature matching loss.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_mels=128, ndf=64):\n",
    "        super().__init__()\n",
    "\n",
    "        # Conv layers with spectral normalization for stability\n",
    "        self.conv1 = nn.utils.spectral_norm(\n",
    "            nn.Conv2d(2, ndf, kernel_size=4, stride=2, padding=1)  # 2 channels: corrupted + reconstruction\n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.utils.spectral_norm(\n",
    "            nn.Conv2d(ndf, ndf * 2, kernel_size=4, stride=2, padding=1)\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(ndf * 2)\n",
    "        \n",
    "        self.conv3 = nn.utils.spectral_norm(\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, kernel_size=4, stride=2, padding=1)\n",
    "        )\n",
    "        self.bn3 = nn.BatchNorm2d(ndf * 4)\n",
    "        \n",
    "        self.conv4 = nn.utils.spectral_norm(\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, kernel_size=4, stride=1, padding=1)\n",
    "        )\n",
    "        self.bn4 = nn.BatchNorm2d(ndf * 8)\n",
    "        \n",
    "        # Output: probability map (PatchGAN)\n",
    "        self.conv5 = nn.Conv2d(ndf * 8, 1, kernel_size=4, stride=1, padding=1)\n",
    "        \n",
    "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
    "        \n",
    "    def forward(self, corrupted, reconstruction):\n",
    "        \"\"\"\n",
    "        corrupted: (batch, seq_len, n_mels)\n",
    "        reconstruction: (batch, seq_len, n_mels)\n",
    "        \n",
    "        Returns:\n",
    "        - output: (batch, h, w) probability map\n",
    "        - features: list of intermediate features for feature matching\n",
    "        \"\"\"\n",
    "        # Stack corrupted and reconstruction as channels\n",
    "        # (batch, seq_len, n_mels) -> (batch, 1, seq_len, n_mels)\n",
    "        corrupted = corrupted.unsqueeze(1)\n",
    "        reconstruction = reconstruction.unsqueeze(1)\n",
    "        \n",
    "        # Concatenate along channel dimension\n",
    "        x = torch.cat([corrupted, reconstruction], dim=1)  # (batch, 2, seq_len, n_mels)\n",
    "        \n",
    "        features = []\n",
    "        \n",
    "        # Layer 1\n",
    "        x = self.conv1(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        features.append(x)\n",
    "        \n",
    "        # Layer 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        features.append(x)\n",
    "        \n",
    "        # Layer 3\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        features.append(x)\n",
    "        \n",
    "        # Layer 4\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        features.append(x)\n",
    "        \n",
    "        # Output\n",
    "        x = self.conv5(x)\n",
    "        \n",
    "        return x, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class PacketLossDataset(Dataset):\n",
    "    \"\"\"Simulates actual packet loss (dropped time frames).\"\"\"\n",
    "    \n",
    "    def __init__(self, audio_dir, sample_rate=16000, n_mels=128, \n",
    "                 segment_length=128, loss_min=0.1, loss_max=0.5):\n",
    "        self.audio_dir = Path(audio_dir)\n",
    "        self.audio_files = list(self.audio_dir.glob(\"*.wav\"))\n",
    "        \n",
    "        if len(self.audio_files) == 0:\n",
    "            raise ValueError(f\"No audio files found in {audio_dir}\")\n",
    "        \n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_mels = n_mels\n",
    "        self.segment_length = segment_length\n",
    "        self.loss_min = loss_min\n",
    "        self.loss_max = loss_max\n",
    "        \n",
    "        self.mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=sample_rate, n_fft=1024, hop_length=256, n_mels=n_mels\n",
    "        )\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.audio_files) * 15\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_idx = idx % len(self.audio_files)\n",
    "        audio_file = self.audio_files[file_idx]\n",
    "        \n",
    "        waveform, sr = torchaudio.load(audio_file)\n",
    "        \n",
    "        if sr != self.sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.sample_rate)\n",
    "            waveform = resampler(waveform)\n",
    "        \n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = waveform.mean(dim=0, keepdim=True)\n",
    "        \n",
    "        mel_spec = self.mel_transform(waveform)\n",
    "        mel_spec = mel_spec.squeeze(0).t()\n",
    "        mel_spec = torch.log(mel_spec + 1e-9)\n",
    "        mel_spec = (mel_spec - mel_spec.mean()) / (mel_spec.std() + 1e-8)\n",
    "        mel_spec = torch.clamp(mel_spec, -3, 3)\n",
    "        \n",
    "        if mel_spec.shape[0] > self.segment_length:\n",
    "            start_idx = np.random.randint(0, mel_spec.shape[0] - self.segment_length)\n",
    "            mel_spec = mel_spec[start_idx:start_idx + self.segment_length]\n",
    "        else:\n",
    "            pad_length = self.segment_length - mel_spec.shape[0]\n",
    "            mel_spec = F.pad(mel_spec, (0, 0, 0, pad_length))\n",
    "        \n",
    "        clean = mel_spec.clone()\n",
    "        \n",
    "        loss_rate = np.random.uniform(self.loss_min, self.loss_max)\n",
    "        mask = torch.rand(self.segment_length) < loss_rate\n",
    "        \n",
    "        corrupted = clean.clone()\n",
    "        corrupted[mask] = 0\n",
    "        \n",
    "        return {\n",
    "            'corrupted': corrupted,\n",
    "            'clean': clean,\n",
    "            'mask': mask,\n",
    "            'loss_rate': loss_rate\n",
    "        }\n",
    "\n",
    "\n",
    "# GAN Loss Functions\n",
    "def adversarial_loss_g(discriminator_output, real_label=0.9):\n",
    "    \"\"\"Generator wants discriminator to output real_label (smoothed).\"\"\"\n",
    "    return F.binary_cross_entropy_with_logits(\n",
    "        discriminator_output, \n",
    "        torch.full_like(discriminator_output, real_label)\n",
    "    )\n",
    "\n",
    "\n",
    "def adversarial_loss_d(real_output, fake_output, real_label=0.9, fake_label=0.0):\n",
    "    \"\"\"Discriminator with one-sided label smoothing.\"\"\"\n",
    "    real_loss = F.binary_cross_entropy_with_logits(\n",
    "        real_output, \n",
    "        torch.full_like(real_output, real_label)  # Smoothed real labels\n",
    "    )\n",
    "    fake_loss = F.binary_cross_entropy_with_logits(\n",
    "        fake_output, \n",
    "        torch.full_like(fake_output, fake_label)  # Hard fake labels\n",
    "    )\n",
    "    return (real_loss + fake_loss) / 2\n",
    "\n",
    "\n",
    "def feature_matching_loss(real_features, fake_features):\n",
    "    \"\"\"Match intermediate discriminator features.\"\"\"\n",
    "    loss = 0\n",
    "    for real_feat, fake_feat in zip(real_features, fake_features):\n",
    "        loss += F.l1_loss(fake_feat, real_feat.detach())\n",
    "    return loss / len(real_features)\n",
    "\n",
    "\n",
    "# Training\n",
    "def train_epoch_gan(generator, discriminator, dataloader, optimizer_g, optimizer_d, device, config):\n",
    "    \"\"\"Train for one epoch with GAN losses.\"\"\"\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    \n",
    "    total_g_loss = 0\n",
    "    total_d_loss = 0\n",
    "    total_l1_loss = 0\n",
    "    total_adv_loss_g = 0\n",
    "    total_fm_loss = 0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=\"Training GAN\")\n",
    "    for i, batch in enumerate(pbar):\n",
    "        corrupted = batch['corrupted'].to(device)\n",
    "        clean = batch['clean'].to(device)\n",
    "        mask = batch['mask'].to(device)\n",
    "        \n",
    "        batch_size = corrupted.size(0)\n",
    "        \n",
    "        # Train Discriminator (every 2 iterations to give generator a chance)\n",
    "        if i % 2 == 0:  # Train D every other iteration\n",
    "            optimizer_d.zero_grad()\n",
    "            \n",
    "            # Generate fake\n",
    "            with torch.no_grad():\n",
    "                fake = generator(corrupted, mask)\n",
    "            \n",
    "            # Discriminator on real (with label smoothing)\n",
    "            real_output, _ = discriminator(corrupted, clean)\n",
    "            \n",
    "            # Discriminator on fake\n",
    "            fake_output, _ = discriminator(corrupted, fake.detach())\n",
    "            \n",
    "            # Discriminator loss with label smoothing\n",
    "            d_loss = adversarial_loss_d(\n",
    "                real_output, fake_output, \n",
    "                real_label=config['real_label'], \n",
    "                fake_label=config['fake_label']\n",
    "            )\n",
    "            \n",
    "            d_loss.backward()\n",
    "            optimizer_d.step()\n",
    "            \n",
    "            total_d_loss += d_loss.item()\n",
    "        \n",
    "        # Train Generator (every iteration)\n",
    "        optimizer_g.zero_grad()\n",
    "        \n",
    "        # Generate fake\n",
    "        fake = generator(corrupted, mask)\n",
    "        \n",
    "        # L1 loss\n",
    "        l1_loss = F.l1_loss(fake, clean)\n",
    "        \n",
    "        # Adversarial loss\n",
    "        fake_output, fake_features = discriminator(corrupted, fake)\n",
    "        adv_loss_g = adversarial_loss_g(fake_output, real_label=config['real_label'])\n",
    "        \n",
    "        # Feature matching loss\n",
    "        _, real_features = discriminator(corrupted, clean)\n",
    "        fm_loss = feature_matching_loss(real_features, fake_features)\n",
    "        \n",
    "        # Total generator loss\n",
    "        g_loss = (\n",
    "            config['lambda_l1'] * l1_loss +\n",
    "            config['lambda_adv'] * adv_loss_g +\n",
    "            config['lambda_fm'] * fm_loss\n",
    "        )\n",
    "        \n",
    "        g_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(generator.parameters(), max_norm=1.0)\n",
    "        optimizer_g.step()\n",
    "        \n",
    "        # Accumulate losses\n",
    "        total_g_loss += g_loss.item()\n",
    "        total_l1_loss += l1_loss.item()\n",
    "        total_adv_loss_g += adv_loss_g.item()\n",
    "        total_fm_loss += fm_loss.item()\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'G': f'{g_loss.item():.3f}',\n",
    "            'D': f'{d_loss.item() if i % 2 == 0 else 0:.3f}',\n",
    "            'L1': f'{l1_loss.item():.3f}',\n",
    "        })\n",
    "    \n",
    "    # Average over full dataloader length for G, but only updates for D\n",
    "    num_d_updates = len(dataloader) // 2\n",
    "    \n",
    "    return {\n",
    "        'g_loss': total_g_loss / len(dataloader),\n",
    "        'd_loss': total_d_loss / max(num_d_updates, 1),\n",
    "        'l1_loss': total_l1_loss / len(dataloader),\n",
    "        'adv_loss_g': total_adv_loss_g / len(dataloader),\n",
    "        'fm_loss': total_fm_loss / len(dataloader),\n",
    "    }\n",
    "\n",
    "\n",
    "def validate_gan(generator, dataloader, device):\n",
    "    \"\"\"Validate generator.\"\"\"\n",
    "    generator.eval()\n",
    "    total_loss = 0\n",
    "    total_l1 = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Validation\"):\n",
    "            corrupted = batch['corrupted'].to(device)\n",
    "            clean = batch['clean'].to(device)\n",
    "            mask = batch['mask'].to(device)\n",
    "            \n",
    "            output = generator(corrupted, mask)\n",
    "            \n",
    "            l1 = F.l1_loss(output, clean)\n",
    "            total_loss += l1.item()\n",
    "            \n",
    "            # Also compute masked L1\n",
    "            if mask.any():\n",
    "                masked_l1 = F.l1_loss(output[mask.unsqueeze(-1).expand_as(output)], \n",
    "                                     clean[mask.unsqueeze(-1).expand_as(clean)])\n",
    "                total_l1 += masked_l1.item()\n",
    "    \n",
    "    return total_loss / len(dataloader), total_l1 / len(dataloader)\n",
    "\n",
    "\n",
    "def load_model_weights(generator, discriminator, optimizer_g, optimizer_d, device):\n",
    "    checkpoint_path = CONFIG['input_model_path']\n",
    "    print(f\"Loading checkpoint from: {checkpoint_path}\")\n",
    "    \n",
    "    # Load to CPU first to avoid GPU OOM if mapping is weird, then map to device. Alternatively restart kernel if issues persist.\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "    # Load model weights\n",
    "    generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "    discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "\n",
    "    # Load optimizer states\n",
    "    optimizer_g.load_state_dict(checkpoint['optimizer_g_state_dict'])\n",
    "    optimizer_d.load_state_dict(checkpoint['optimizer_d_state_dict'])\n",
    "\n",
    "    # Resume epoch and val_loss\n",
    "    start_epoch = checkpoint.get('epoch', 0) + 1\n",
    "    \n",
    "    # Handle case where val_loss might be None or missing in old checkpoints\n",
    "    val_loss = checkpoint.get('val_loss', float('inf'))\n",
    "\n",
    "    print(f\"Resuming training from epoch {start_epoch}, previous val_loss={val_loss}\")\n",
    "\n",
    "    return start_epoch, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T15:48:43.797869Z",
     "iopub.status.busy": "2025-12-20T15:48:43.797022Z",
     "iopub.status.idle": "2025-12-20T15:48:43.820192Z",
     "shell.execute_reply": "2025-12-20T15:48:43.819589Z",
     "shell.execute_reply.started": "2025-12-20T15:48:43.797820Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "class DownloadProgressBar(tqdm):\n",
    "    def update_to(self, b=1, bsize=1, tsize=None):\n",
    "        if tsize is not None:\n",
    "            self.total = tsize\n",
    "        self.update(b * bsize - self.n)\n",
    "\n",
    "\n",
    "def download_ljspeech(data_dir, url, num_files=1000):\n",
    "    data_dir = Path(data_dir)\n",
    "    data_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"Downloading LJSpeech Dataset\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    tar_path = data_dir.parent / \"LJSpeech-1.1.tar.bz2\"\n",
    "    extract_dir = data_dir.parent / \"LJSpeech-1.1\"\n",
    "    \n",
    "    if (extract_dir / \"wavs\").exists() and len(list((extract_dir / \"wavs\").glob(\"*.wav\"))) > 0:\n",
    "        print(f\"✓ Dataset already exists\")\n",
    "        wav_files = sorted(list((extract_dir / \"wavs\").glob(\"*.wav\")))[:num_files]\n",
    "        \n",
    "        import shutil\n",
    "        print(f\"Copying {len(wav_files)} files...\")\n",
    "        for i, wav_file in enumerate(wav_files):\n",
    "            shutil.copy(wav_file, data_dir / wav_file.name)\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"  Copied {i + 1}/{len(wav_files)}\")\n",
    "        \n",
    "        print(f\"✓ Ready: {len(wav_files)} files\")\n",
    "        return\n",
    "    \n",
    "    if not tar_path.exists():\n",
    "        print(f\"\\nDownloading (~2.6 GB)...\")\n",
    "        with DownloadProgressBar(unit='B', unit_scale=True, miniters=1) as t:\n",
    "            urllib.request.urlretrieve(url, filename=tar_path, reporthook=t.update_to)\n",
    "    \n",
    "    print(f\"\\nExtracting...\")\n",
    "    with tarfile.open(tar_path, 'r:bz2') as tar:\n",
    "        tar.extractall(data_dir.parent)\n",
    "    \n",
    "    wav_files = sorted(list((extract_dir / \"wavs\").glob(\"*.wav\")))[:num_files]\n",
    "    \n",
    "    import shutil\n",
    "    for i, wav_file in enumerate(wav_files):\n",
    "        shutil.copy(wav_file, data_dir / wav_file.name)\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"  Copied {i + 1}/{len(wav_files)}\")\n",
    "    \n",
    "    tar_path.unlink()\n",
    "    print(f\"\\n✓ Ready: {len(wav_files)} files\")\n",
    "\n",
    "# Inference (Same as before but now uses generator)\n",
    "def reconstruct_and_visualize(generator, audio_path, loss_rate, device, output_dir):\n",
    "    generator.eval()\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    waveform, sr = torchaudio.load(audio_path)\n",
    "    mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=16000, n_fft=1024, hop_length=256, n_mels=128\n",
    "    )\n",
    "    \n",
    "    if sr != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(sr, 16000)\n",
    "        waveform = resampler(waveform)\n",
    "    \n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "    \n",
    "    mel_spec = mel_transform(waveform).squeeze(0).t()\n",
    "    mel_spec = torch.log(mel_spec + 1e-9)\n",
    "    \n",
    "    mel_mean = mel_spec.mean()\n",
    "    mel_std = mel_spec.std()\n",
    "    \n",
    "    mel_spec = (mel_spec - mel_mean) / (mel_std + 1e-8)\n",
    "    mel_spec = torch.clamp(mel_spec, -3, 3)\n",
    "    \n",
    "    max_len = 500\n",
    "    if mel_spec.shape[0] > max_len:\n",
    "        mel_spec = mel_spec[:max_len]\n",
    "    \n",
    "    clean = mel_spec.clone()\n",
    "    \n",
    "    mask = torch.rand(mel_spec.shape[0]) < loss_rate\n",
    "    corrupted = clean.clone()\n",
    "    corrupted[mask] = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        corrupted_input = corrupted.unsqueeze(0).to(device)\n",
    "        mask_input = mask.unsqueeze(0).to(device)\n",
    "        reconstructed = generator(corrupted_input, mask_input)\n",
    "        reconstructed = reconstructed.squeeze(0).cpu()\n",
    "    \n",
    "    # Convert to audio using Griffin-Lim\n",
    "    inverse_mel = torchaudio.transforms.InverseMelScale(n_stft=513, n_mels=80, sample_rate=16000)\n",
    "    # griffin_lim = torchaudio.transforms.GriffinLim(n_fft=1024, hop_length=256, n_iter=64)\n",
    "    griffin_lim = torchaudio.transforms.GriffinLim(n_fft=1024, hop_length=256, n_iter=128)  # Increase from 64 to 128 for better audio quality, but still need a vocoder\n",
    "    \n",
    "    def mel_to_audio(mel, mel_mean, mel_std):\n",
    "        mel = mel * mel_std + mel_mean\n",
    "        mel = torch.exp(mel) - 1e-9\n",
    "        mel = mel.clamp(min=0).t().unsqueeze(0)\n",
    "        linear = inverse_mel(mel)\n",
    "        audio = griffin_lim(linear)\n",
    "        return audio\n",
    "    \n",
    "    clean_audio = mel_to_audio(clean, mel_mean, mel_std)\n",
    "    corrupted_audio = mel_to_audio(corrupted, mel_mean, mel_std)\n",
    "    reconstructed_audio = mel_to_audio(reconstructed, mel_mean, mel_std)\n",
    "    \n",
    "    base_name = Path(audio_path).stem\n",
    "    loss_pct = int(loss_rate * 100)\n",
    "    \n",
    "    torchaudio.save(str(output_dir / f\"{base_name}_clean_gan.wav\"), clean_audio, 16000)\n",
    "    torchaudio.save(str(output_dir / f\"{base_name}_corrupted_{loss_pct}pct_gan.wav\"), corrupted_audio, 16000)\n",
    "    torchaudio.save(str(output_dir / f\"{base_name}_reconstructed_{loss_pct}pct_gan.wav\"), reconstructed_audio, 16000)\n",
    "    \n",
    "    # Metrics\n",
    "    if mask.sum() > 0:\n",
    "        l1_corrupted = F.l1_loss(corrupted[mask], clean[mask]).item()\n",
    "        l1_reconstructed = F.l1_loss(reconstructed[mask], clean[mask]).item()\n",
    "        improvement = ((l1_corrupted - l1_reconstructed) / l1_corrupted) * 100\n",
    "    else:\n",
    "        l1_corrupted = 0.0\n",
    "        l1_reconstructed = 0.0\n",
    "        improvement = 0.0\n",
    "    \n",
    "    l1_full = F.l1_loss(reconstructed, clean).item()\n",
    "    signal_power = (clean ** 2).mean().item()\n",
    "    noise_power = ((reconstructed - clean) ** 2).mean().item()\n",
    "    snr = 10 * np.log10(signal_power / (noise_power + 1e-8))\n",
    "    \n",
    "    metrics = {\n",
    "        'l1_corrupted_masked': float(l1_corrupted),\n",
    "        'l1_reconstructed_masked': float(l1_reconstructed),\n",
    "        'improvement_pct': float(improvement),\n",
    "        'l1_full': float(l1_full),\n",
    "        'snr_db': float(snr),\n",
    "        'loss_rate': loss_rate,\n",
    "        'packets_lost': int(mask.sum()),\n",
    "        'total_packets': len(mask),\n",
    "        'model': 'GAN'\n",
    "    }\n",
    "    \n",
    "    with open(output_dir / f\"{base_name}_metrics_{loss_pct}pct_gan.json\", 'w') as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(4, 1, figsize=(14, 12))\n",
    "    \n",
    "    axes[0].imshow(clean.numpy().T, aspect='auto', origin='lower', cmap='viridis', vmin=-2, vmax=2)\n",
    "    axes[0].set_title('Clean Audio', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    axes[1].imshow(corrupted.numpy().T, aspect='auto', origin='lower', cmap='viridis', vmin=-2, vmax=2)\n",
    "    axes[1].set_title(f'Corrupted ({loss_pct}% Packet Loss)', fontsize=14, fontweight='bold')\n",
    "    for i, is_lost in enumerate(mask):\n",
    "        if is_lost:\n",
    "            axes[1].axvline(x=i, color='red', alpha=0.3, linewidth=0.5)\n",
    "    \n",
    "    axes[2].imshow(reconstructed.numpy().T, aspect='auto', origin='lower', cmap='viridis', vmin=-2, vmax=2)\n",
    "    axes[2].set_title(f'Reconstructed (GAN) - SNR: {snr:.1f} dB', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    error = torch.abs(reconstructed - clean)\n",
    "    im = axes[3].imshow(error.numpy().T, aspect='auto', origin='lower', cmap='hot')\n",
    "    axes[3].set_title(f'Error (Improvement: {improvement:.1f}%)', fontsize=14, fontweight='bold')\n",
    "    axes[3].set_xlabel('Time Frames')\n",
    "    plt.colorbar(im, ax=axes[3])\n",
    "    \n",
    "    for ax in axes:\n",
    "        ax.set_ylabel('Mel Bins')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / f\"{base_name}_comparison_{loss_pct}pct_gan.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"\\n✓ {base_name} @ {loss_pct}% loss\")\n",
    "    print(f\"  L1 (masked): {l1_corrupted:.4f} → {l1_reconstructed:.4f}\")\n",
    "    print(f\"  Improvement: {improvement:.1f}%\")\n",
    "    print(f\"  SNR: {snr:.1f} dB\")\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T15:48:46.053848Z",
     "iopub.status.busy": "2025-12-20T15:48:46.053132Z",
     "iopub.status.idle": "2025-12-20T15:48:46.071469Z",
     "shell.execute_reply": "2025-12-20T15:48:46.070661Z",
     "shell.execute_reply.started": "2025-12-20T15:48:46.053822Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def main(resume_training=False):\n",
    "    print(\"=\"*60)\n",
    "    print(\"Voice Packet Reconstruction - GAN VERSION\")\n",
    "    print(\"Sharp, Realistic Reconstruction!\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"\\nDevice: {device}\")\n",
    "    \n",
    "    # Download data\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 1: Data\")\n",
    "    print(\"=\"*60)\n",
    "    download_ljspeech(CONFIG['data_dir'], CONFIG['ljspeech_url'], CONFIG['num_files_to_use'])\n",
    "    \n",
    "    # Dataset\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 2: Dataset\")\n",
    "    print(\"=\"*60)\n",
    "    full_dataset = PacketLossDataset(\n",
    "        audio_dir=CONFIG['data_dir'],\n",
    "        loss_min=CONFIG['packet_loss_min'],\n",
    "        loss_max=CONFIG['packet_loss_max']\n",
    "    )\n",
    "    \n",
    "    train_size = int(0.9 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=2)\n",
    "    \n",
    "    print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}\")\n",
    "\n",
    "    start_epoch = 0\n",
    "    \n",
    "    # Models\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 3: Models\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    generator = PacketLossReconstructor(\n",
    "        n_mels=128, hidden_dim=512, num_layers=6, dropout=0.3, max_len=500\n",
    "    ).to(device)\n",
    "    \n",
    "    discriminator = PatchGANDiscriminator(\n",
    "        n_mels=128, ndf=64\n",
    "    ).to(device)\n",
    "    \n",
    "    num_params_g = sum(p.numel() for p in generator.parameters() if p.requires_grad)\n",
    "    num_params_d = sum(p.numel() for p in discriminator.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"Generator parameters: {num_params_g:,}\")\n",
    "    print(f\"Discriminator parameters: {num_params_d:,}\")\n",
    "    print(f\"Loss: L1 + Adversarial + Feature Matching\")\n",
    "    \n",
    "    # Optimizers\n",
    "    optimizer_g = torch.optim.Adam(\n",
    "        generator.parameters(), \n",
    "        lr=CONFIG['learning_rate_g'], \n",
    "        betas=(0.5, 0.999)\n",
    "    )\n",
    "    optimizer_d = torch.optim.Adam(\n",
    "        discriminator.parameters(), \n",
    "        lr=CONFIG['learning_rate_d'], \n",
    "        betas=(0.5, 0.999)\n",
    "    )\n",
    "    \n",
    "    # Training\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 4: Training GAN\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    Path(CONFIG['checkpoint_dir']).mkdir(parents=True, exist_ok=True)\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    if resume_training:\n",
    "        print(\"\\nResuming from checkpoint...\")\n",
    "        # Pass the instances created above into the function\n",
    "        start_epoch, resume_loss = load_model_weights(\n",
    "            generator, discriminator, optimizer_g, optimizer_d, device\n",
    "        )\n",
    "        if resume_loss is not None:\n",
    "            best_val_loss = resume_loss\n",
    "\n",
    "    # Schedulers\n",
    "    scheduler_g = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_g, mode='min', factor=0.5, patience=3)\n",
    "    scheduler_d = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_d, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "    patience_counter = 0\n",
    "    history = {'g_loss': [], 'd_loss': [], 'val_loss': []}\n",
    "\n",
    "    if start_epoch >= CONFIG['num_epochs']:\n",
    "        print(f\"Training already completed ({start_epoch} epochs). Increase num_epochs to continue.\")\n",
    "        return\n",
    "    \n",
    "    # for epoch in range(CONFIG['num_epochs']):\n",
    "    for epoch in range(start_epoch, CONFIG['num_epochs']):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{CONFIG['num_epochs']}\")\n",
    "        \n",
    "        train_metrics = train_epoch_gan(\n",
    "            generator, discriminator, train_loader, \n",
    "            optimizer_g, optimizer_d, device, CONFIG\n",
    "        )\n",
    "        \n",
    "        val_loss, val_masked = validate_gan(generator, val_loader, device)\n",
    "        \n",
    "        scheduler_g.step(val_loss)\n",
    "        scheduler_d.step(val_loss)\n",
    "        \n",
    "        print(f\"Generator Loss: {train_metrics['g_loss']:.4f} \"\n",
    "              f\"(L1: {train_metrics['l1_loss']:.4f}, \"\n",
    "              f\"Adv: {train_metrics['adv_loss_g']:.4f}, \"\n",
    "              f\"FM: {train_metrics['fm_loss']:.4f})\")\n",
    "        print(f\"Discriminator Loss: {train_metrics['d_loss']:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f} (Masked: {val_masked:.4f})\")\n",
    "        \n",
    "        history['g_loss'].append(train_metrics['g_loss'])\n",
    "        history['d_loss'].append(train_metrics['d_loss'])\n",
    "        history['val_loss'].append(val_loss)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'generator_state_dict': generator.state_dict(),\n",
    "                'discriminator_state_dict': discriminator.state_dict(),\n",
    "                'optimizer_g_state_dict': optimizer_g.state_dict(),\n",
    "                'optimizer_d_state_dict': optimizer_d.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "            }, Path(CONFIG['checkpoint_dir']) / 'best_model_gan.pt')\n",
    "            print(\"✓ Saved!\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= CONFIG['early_stopping_patience']:\n",
    "                print(f\"\\nEarly stop at epoch {epoch + 1}\")\n",
    "                break\n",
    "    \n",
    "    # Save history\n",
    "    with open(Path(CONFIG['checkpoint_dir']) / 'training_history_gan.json', 'w') as f:\n",
    "        json.dump(history, f, indent=2)\n",
    "    \n",
    "    # Plot training\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    ax1.plot(history['g_loss'], label='Generator')\n",
    "    ax1.plot(history['d_loss'], label='Discriminator')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Training Losses')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax2.plot(history['val_loss'], label='Validation L1')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.set_title('Validation Loss')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(Path(CONFIG['checkpoint_dir']) / 'training_curves_gan.png', dpi=150)\n",
    "    plt.close()\n",
    "    \n",
    "    # Inference\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 5: Inference\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    test_files = sorted(list(Path(CONFIG['data_dir']).glob(\"*.wav\")))[:5]\n",
    "    test_loss_rates = [0.2, 0.3, 0.4, 0.5]\n",
    "    \n",
    "    for audio_file in test_files:\n",
    "        for loss_rate in test_loss_rates:\n",
    "            reconstruct_and_visualize(generator, str(audio_file), loss_rate, device, CONFIG['output_dir'])\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"✓ DONE!\")\n",
    "    print(f\"Best val loss: {best_val_loss:.4f}\")\n",
    "    print(f\"Outputs: {CONFIG['output_dir']}\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T15:48:47.947958Z",
     "iopub.status.busy": "2025-12-20T15:48:47.947193Z",
     "iopub.status.idle": "2025-12-20T16:18:13.355430Z",
     "shell.execute_reply": "2025-12-20T16:18:13.354527Z",
     "shell.execute_reply.started": "2025-12-20T15:48:47.947930Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Voice Packet Reconstruction - GAN VERSION\n",
      "Sharp, Realistic Reconstruction!\n",
      "============================================================\n",
      "\n",
      "Device: cuda\n",
      "\n",
      "============================================================\n",
      "STEP 1: Data\n",
      "============================================================\n",
      "============================================================\n",
      "Downloading LJSpeech Dataset\n",
      "============================================================\n",
      "✓ Dataset already exists\n",
      "Copying 1000 files...\n",
      "  Copied 100/1000\n",
      "  Copied 200/1000\n",
      "  Copied 300/1000\n",
      "  Copied 400/1000\n",
      "  Copied 500/1000\n",
      "  Copied 600/1000\n",
      "  Copied 700/1000\n",
      "  Copied 800/1000\n",
      "  Copied 900/1000\n",
      "  Copied 1000/1000\n",
      "✓ Ready: 1000 files\n",
      "\n",
      "============================================================\n",
      "STEP 2: Dataset\n",
      "============================================================\n",
      "Train: 13500, Val: 1500\n",
      "\n",
      "============================================================\n",
      "STEP 3: Models\n",
      "============================================================\n",
      "Generator parameters: 2,404,784\n",
      "Discriminator parameters: 2,765,505\n",
      "Loss: L1 + Adversarial + Feature Matching\n",
      "\n",
      "============================================================\n",
      "STEP 4: Training GAN\n",
      "============================================================\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training GAN: 100%|██████████| 844/844 [02:34<00:00,  5.47it/s, G=41.822, D=0.000, L1=0.380]\n",
      "Validation: 100%|██████████| 94/94 [00:16<00:00,  5.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator Loss: 46.3991 (L1: 0.4259, Adv: 4.0032, FM: 0.3408)\n",
      "Discriminator Loss: 0.2164\n",
      "Val Loss: 0.3244 (Masked: 0.7062)\n",
      "✓ Saved!\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training GAN: 100%|██████████| 844/844 [02:30<00:00,  5.62it/s, G=37.854, D=0.000, L1=0.340]\n",
      "Validation: 100%|██████████| 94/94 [00:17<00:00,  5.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator Loss: 40.4599 (L1: 0.3640, Adv: 6.0977, FM: 0.3449)\n",
      "Discriminator Loss: 0.1651\n",
      "Val Loss: 0.3104 (Masked: 0.7093)\n",
      "✓ Saved!\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training GAN: 100%|██████████| 844/844 [02:31<00:00,  5.56it/s, G=38.803, D=0.000, L1=0.346]\n",
      "Validation: 100%|██████████| 94/94 [00:17<00:00,  5.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator Loss: 39.7091 (L1: 0.3554, Adv: 6.9058, FM: 0.3475)\n",
      "Discriminator Loss: 0.1637\n",
      "Val Loss: 0.3032 (Masked: 0.7079)\n",
      "✓ Saved!\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training GAN: 100%|██████████| 844/844 [02:28<00:00,  5.70it/s, G=40.722, D=0.000, L1=0.363]\n",
      "Validation: 100%|██████████| 94/94 [00:16<00:00,  5.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator Loss: 39.3887 (L1: 0.3516, Adv: 7.4033, FM: 0.3490)\n",
      "Discriminator Loss: 0.1633\n",
      "Val Loss: 0.3018 (Masked: 0.7084)\n",
      "✓ Saved!\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training GAN: 100%|██████████| 844/844 [02:31<00:00,  5.57it/s, G=34.839, D=0.000, L1=0.307]\n",
      "Validation: 100%|██████████| 94/94 [00:16<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator Loss: 39.0139 (L1: 0.3473, Adv: 7.8269, FM: 0.3505)\n",
      "Discriminator Loss: 0.1631\n",
      "Val Loss: 0.2942 (Masked: 0.7082)\n",
      "✓ Saved!\n",
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training GAN: 100%|██████████| 844/844 [02:31<00:00,  5.58it/s, G=42.492, D=0.000, L1=0.379]\n",
      "Validation: 100%|██████████| 94/94 [00:17<00:00,  5.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator Loss: 38.7053 (L1: 0.3437, Adv: 8.1797, FM: 0.3520)\n",
      "Discriminator Loss: 0.1629\n",
      "Val Loss: 0.2918 (Masked: 0.7077)\n",
      "✓ Saved!\n",
      "\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training GAN: 100%|██████████| 844/844 [02:33<00:00,  5.50it/s, G=34.505, D=0.000, L1=0.303]\n",
      "Validation: 100%|██████████| 94/94 [00:17<00:00,  5.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator Loss: 38.4618 (L1: 0.3410, Adv: 8.4388, FM: 0.3515)\n",
      "Discriminator Loss: 0.1628\n",
      "Val Loss: 0.2933 (Masked: 0.7078)\n",
      "\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training GAN: 100%|██████████| 844/844 [02:34<00:00,  5.47it/s, G=34.551, D=0.000, L1=0.303]\n",
      "Validation: 100%|██████████| 94/94 [00:17<00:00,  5.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator Loss: 38.1072 (L1: 0.3377, Adv: 8.5603, FM: 0.3486)\n",
      "Discriminator Loss: 0.1628\n",
      "Val Loss: 0.2792 (Masked: 0.6710)\n",
      "✓ Saved!\n",
      "\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training GAN: 100%|██████████| 844/844 [02:31<00:00,  5.58it/s, G=33.535, D=0.000, L1=0.303]\n",
      "Validation: 100%|██████████| 94/94 [00:16<00:00,  5.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator Loss: 34.9293 (L1: 0.3189, Adv: 5.8613, FM: 0.2458)\n",
      "Discriminator Loss: 0.2052\n",
      "Val Loss: 0.2502 (Masked: 0.5811)\n",
      "✓ Saved!\n",
      "\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training GAN: 100%|██████████| 844/844 [02:32<00:00,  5.53it/s, G=34.069, D=0.000, L1=0.307]\n",
      "Validation: 100%|██████████| 94/94 [00:17<00:00,  5.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator Loss: 33.1902 (L1: 0.2993, Adv: 7.3668, FM: 0.2524)\n",
      "Discriminator Loss: 0.1634\n",
      "Val Loss: 0.2330 (Masked: 0.5131)\n",
      "✓ Saved!\n",
      "\n",
      "============================================================\n",
      "STEP 5: Inference\n",
      "============================================================\n",
      "\n",
      "✓ LJ001-0001 @ 20% loss\n",
      "  L1 (masked): 0.8596 → 0.7512\n",
      "  Improvement: 12.6%\n",
      "  SNR: 7.2 dB\n",
      "\n",
      "✓ LJ001-0001 @ 30% loss\n",
      "  L1 (masked): 0.8534 → 0.7464\n",
      "  Improvement: 12.5%\n",
      "  SNR: 5.4 dB\n",
      "\n",
      "✓ LJ001-0001 @ 40% loss\n",
      "  L1 (masked): 0.7999 → 0.6874\n",
      "  Improvement: 14.1%\n",
      "  SNR: 5.1 dB\n",
      "\n",
      "✓ LJ001-0001 @ 50% loss\n",
      "  L1 (masked): 0.8408 → 0.7297\n",
      "  Improvement: 13.2%\n",
      "  SNR: 4.1 dB\n",
      "\n",
      "✓ LJ001-0002 @ 20% loss\n",
      "  L1 (masked): 0.7979 → 0.4312\n",
      "  Improvement: 46.0%\n",
      "  SNR: 10.5 dB\n",
      "\n",
      "✓ LJ001-0002 @ 30% loss\n",
      "  L1 (masked): 0.8033 → 0.5049\n",
      "  Improvement: 37.2%\n",
      "  SNR: 8.4 dB\n",
      "\n",
      "✓ LJ001-0002 @ 40% loss\n",
      "  L1 (masked): 0.8083 → 0.4614\n",
      "  Improvement: 42.9%\n",
      "  SNR: 8.4 dB\n",
      "\n",
      "✓ LJ001-0002 @ 50% loss\n",
      "  L1 (masked): 0.8103 → 0.4954\n",
      "  Improvement: 38.9%\n",
      "  SNR: 7.2 dB\n",
      "\n",
      "✓ LJ001-0003 @ 20% loss\n",
      "  L1 (masked): 0.7745 → 0.6568\n",
      "  Improvement: 15.2%\n",
      "  SNR: 8.0 dB\n",
      "\n",
      "✓ LJ001-0003 @ 30% loss\n",
      "  L1 (masked): 0.7787 → 0.6686\n",
      "  Improvement: 14.1%\n",
      "  SNR: 6.2 dB\n",
      "\n",
      "✓ LJ001-0003 @ 40% loss\n",
      "  L1 (masked): 0.8041 → 0.6962\n",
      "  Improvement: 13.4%\n",
      "  SNR: 4.7 dB\n",
      "\n",
      "✓ LJ001-0003 @ 50% loss\n",
      "  L1 (masked): 0.8073 → 0.6990\n",
      "  Improvement: 13.4%\n",
      "  SNR: 4.2 dB\n",
      "\n",
      "✓ LJ001-0004 @ 20% loss\n",
      "  L1 (masked): 0.8051 → 0.6118\n",
      "  Improvement: 24.0%\n",
      "  SNR: 8.2 dB\n",
      "\n",
      "✓ LJ001-0004 @ 30% loss\n",
      "  L1 (masked): 0.8233 → 0.6602\n",
      "  Improvement: 19.8%\n",
      "  SNR: 6.9 dB\n",
      "\n",
      "✓ LJ001-0004 @ 40% loss\n",
      "  L1 (masked): 0.8323 → 0.6826\n",
      "  Improvement: 18.0%\n",
      "  SNR: 5.4 dB\n",
      "\n",
      "✓ LJ001-0004 @ 50% loss\n",
      "  L1 (masked): 0.7920 → 0.6894\n",
      "  Improvement: 13.0%\n",
      "  SNR: 3.7 dB\n",
      "\n",
      "✓ LJ001-0005 @ 20% loss\n",
      "  L1 (masked): 0.7709 → 0.6484\n",
      "  Improvement: 15.9%\n",
      "  SNR: 8.1 dB\n",
      "\n",
      "✓ LJ001-0005 @ 30% loss\n",
      "  L1 (masked): 0.7825 → 0.6789\n",
      "  Improvement: 13.2%\n",
      "  SNR: 6.4 dB\n",
      "\n",
      "✓ LJ001-0005 @ 40% loss\n",
      "  L1 (masked): 0.8039 → 0.6966\n",
      "  Improvement: 13.3%\n",
      "  SNR: 4.9 dB\n",
      "\n",
      "✓ LJ001-0005 @ 50% loss\n",
      "  L1 (masked): 0.8244 → 0.7024\n",
      "  Improvement: 14.8%\n",
      "  SNR: 4.2 dB\n",
      "\n",
      "============================================================\n",
      "✓ DONE!\n",
      "Best val loss: 0.2330\n",
      "Outputs: /kaggle/working/outputs\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main(resume_training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T16:25:20.171712Z",
     "iopub.status.busy": "2025-12-20T16:25:20.170934Z",
     "iopub.status.idle": "2025-12-20T16:25:20.411558Z",
     "shell.execute_reply": "2025-12-20T16:25:20.410896Z",
     "shell.execute_reply.started": "2025-12-20T16:25:20.171684Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator parameters: 2,404,784\n",
      "Discriminator parameters: 2,765,505\n",
      "Loss: L1 + Adversarial + Feature Matching\n",
      "Loading checkpoint from: /kaggle/working/checkpoints/best_model_gan.pt\n",
      "\n",
      "Resuming from checkpoint...\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "generator = PacketLossReconstructor(\n",
    "    n_mels=128, hidden_dim=512, num_layers=6, dropout=0.3, max_len=500\n",
    ").to(device)\n",
    "    \n",
    "discriminator = PatchGANDiscriminator(\n",
    "    n_mels=128, ndf=64\n",
    ").to(device)\n",
    "    \n",
    "num_params_g = sum(p.numel() for p in generator.parameters() if p.requires_grad)\n",
    "num_params_d = sum(p.numel() for p in discriminator.parameters() if p.requires_grad)\n",
    "    \n",
    "print(f\"Generator parameters: {num_params_g:,}\")\n",
    "print(f\"Discriminator parameters: {num_params_d:,}\")\n",
    "print(f\"Loss: L1 + Adversarial + Feature Matching\")\n",
    "    \n",
    "# Optimizers\n",
    "optimizer_g = torch.optim.Adam(\n",
    "    generator.parameters(), \n",
    "    lr=CONFIG['learning_rate_g'], \n",
    "    betas=(0.5, 0.999)\n",
    ")\n",
    "optimizer_d = torch.optim.Adam(\n",
    "    discriminator.parameters(), \n",
    "    lr=CONFIG['learning_rate_d'], \n",
    "    betas=(0.5, 0.999)\n",
    ")\n",
    "\n",
    "\n",
    "checkpoint_path = \"/kaggle/working/checkpoints/best_model_gan.pt\"\n",
    "print(f\"Loading checkpoint from: {checkpoint_path}\")\n",
    "    \n",
    "# Load to CPU first to avoid GPU OOM if mapping is weird, then map to device\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "# Load model weights\n",
    "generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "\n",
    "# Load optimizer states\n",
    "optimizer_g.load_state_dict(checkpoint['optimizer_g_state_dict'])\n",
    "optimizer_d.load_state_dict(checkpoint['optimizer_d_state_dict'])\n",
    "    \n",
    "print(\"\\nResuming from checkpoint...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T16:25:29.514082Z",
     "iopub.status.busy": "2025-12-20T16:25:29.513757Z",
     "iopub.status.idle": "2025-12-20T16:25:29.537005Z",
     "shell.execute_reply": "2025-12-20T16:25:29.536278Z",
     "shell.execute_reply.started": "2025-12-20T16:25:29.514059Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_hifigan(device):\n",
    "    \"\"\"Load HiFi-GAN - should work with 80 mels\"\"\"\n",
    "    try:\n",
    "        print(\"Loading HiFi-GAN vocoder...\")\n",
    "        loaded_obj = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_hifigan')\n",
    "        \n",
    "        if isinstance(loaded_obj, tuple):\n",
    "            vocoder = loaded_obj[0]\n",
    "        else:\n",
    "            vocoder = loaded_obj\n",
    "            \n",
    "        vocoder = vocoder.to(device).eval()\n",
    "        print(\"✓ HiFi-GAN loaded successfully!\")\n",
    "        return vocoder, 'hifigan'\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Failed: {e}\")\n",
    "        return None, 'griffin-lim'\n",
    "\n",
    "\n",
    "def mel_to_audio_hifigan(mel_spec, vocoder, device, original_mean, original_std):\n",
    "    \"\"\"\n",
    "    Convert 80-mel spectrogram to audio using HiFi-GAN\n",
    "    \"\"\"\n",
    "    # Denormalize\n",
    "    mel = mel_spec * original_std + original_mean\n",
    "    mel = torch.exp(mel) - 1e-9\n",
    "    mel = mel.clamp(min=0)\n",
    "    \n",
    "    # HiFi-GAN expects (batch, 80, time) - we have (seq_len, 80)\n",
    "    mel = mel.t().unsqueeze(0).to(device)  # (1, 80, seq_len)\n",
    "    \n",
    "    # Generate audio\n",
    "    with torch.no_grad():\n",
    "        audio = vocoder(mel).squeeze(1)\n",
    "    \n",
    "    return audio.cpu()\n",
    "\n",
    "\n",
    "def reconstruct_and_visualize_80mel(model, audio_path, loss_rate, device, output_dir, \n",
    "                                   vocoder=None, vocoder_type='griffin-lim'):\n",
    "    \"\"\"\n",
    "    Inference function for 80-mel model with HiFi-GAN support\n",
    "    \"\"\"\n",
    "    import torchaudio\n",
    "    import matplotlib.pyplot as plt\n",
    "    from pathlib import Path\n",
    "    import json\n",
    "    import torch.nn.functional as F\n",
    "    import numpy as np\n",
    "    \n",
    "    model.eval()\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Load audio\n",
    "    waveform, sr = torchaudio.load(audio_path)\n",
    "    mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=16000, n_fft=1024, hop_length=256, n_mels=80  # 80 mels!\n",
    "    )\n",
    "    \n",
    "    if sr != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(sr, 16000)\n",
    "        waveform = resampler(waveform)\n",
    "    \n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "    \n",
    "    mel_spec = mel_transform(waveform).squeeze(0).t()\n",
    "    mel_spec = torch.log(mel_spec + 1e-9)\n",
    "    \n",
    "    # Save original stats BEFORE normalization\n",
    "    original_mean = mel_spec.mean()\n",
    "    original_std = mel_spec.std()\n",
    "    \n",
    "    # Normalize\n",
    "    mel_spec = (mel_spec - original_mean) / (original_std + 1e-8)\n",
    "    mel_spec = torch.clamp(mel_spec, -3, 3)\n",
    "    \n",
    "    max_len = 500\n",
    "    if mel_spec.shape[0] > max_len:\n",
    "        mel_spec = mel_spec[:max_len]\n",
    "    \n",
    "    clean = mel_spec.clone()\n",
    "    \n",
    "    # Simulate packet loss\n",
    "    mask = torch.rand(mel_spec.shape[0]) < loss_rate\n",
    "    corrupted = clean.clone()\n",
    "    corrupted[mask] = 0\n",
    "    \n",
    "    # Reconstruct\n",
    "    with torch.no_grad():\n",
    "        corrupted_input = corrupted.unsqueeze(0).to(device)\n",
    "        mask_input = mask.unsqueeze(0).to(device)\n",
    "        reconstructed = model(corrupted_input, mask_input)\n",
    "        reconstructed = reconstructed.squeeze(0).cpu()\n",
    "    \n",
    "    # Convert to audio\n",
    "    if vocoder is not None and vocoder_type == 'hifigan':\n",
    "        print(f\"  Using HiFi-GAN vocoder (80 mels - perfect match!)...\")\n",
    "        \n",
    "        clean_audio = mel_to_audio_hifigan(clean, vocoder, device, original_mean, original_std)\n",
    "        corrupted_audio = mel_to_audio_hifigan(corrupted, vocoder, device, original_mean, original_std)\n",
    "        reconstructed_audio = mel_to_audio_hifigan(reconstructed, vocoder, device, original_mean, original_std)\n",
    "        \n",
    "    else:\n",
    "        # Fallback to Griffin-Lim\n",
    "        print(\"  Using Griffin-Lim vocoder...\")\n",
    "        inverse_mel = torchaudio.transforms.InverseMelScale(n_stft=513, n_mels=80, sample_rate=16000)\n",
    "        griffin_lim = torchaudio.transforms.GriffinLim(n_fft=1024, hop_length=256, n_iter=128)\n",
    "        \n",
    "        def mel_to_audio_gl(mel):\n",
    "            mel = mel * original_std + original_mean\n",
    "            mel = torch.exp(mel) - 1e-9\n",
    "            mel = mel.clamp(min=0).t().unsqueeze(0)\n",
    "            linear = inverse_mel(mel)\n",
    "            audio = griffin_lim(linear)\n",
    "            return audio\n",
    "        \n",
    "        clean_audio = mel_to_audio_gl(clean)\n",
    "        corrupted_audio = mel_to_audio_gl(corrupted)\n",
    "        reconstructed_audio = mel_to_audio_gl(reconstructed)\n",
    "    \n",
    "    # Save audio\n",
    "    base_name = Path(audio_path).stem\n",
    "    loss_pct = int(loss_rate * 100)\n",
    "    \n",
    "    suffix = f\"_{vocoder_type}_80mel\"\n",
    "    \n",
    "    torchaudio.save(str(output_dir / f\"{base_name}_clean{suffix}.wav\"), clean_audio, 16000)\n",
    "    torchaudio.save(str(output_dir / f\"{base_name}_corrupted_{loss_pct}pct{suffix}.wav\"), corrupted_audio, 16000)\n",
    "    torchaudio.save(str(output_dir / f\"{base_name}_reconstructed_{loss_pct}pct{suffix}.wav\"), reconstructed_audio, 16000)\n",
    "    \n",
    "    # Metrics\n",
    "    if mask.sum() > 0:\n",
    "        l1_corrupted = F.l1_loss(corrupted[mask], clean[mask]).item()\n",
    "        l1_reconstructed = F.l1_loss(reconstructed[mask], clean[mask]).item()\n",
    "        improvement = ((l1_corrupted - l1_reconstructed) / l1_corrupted) * 100\n",
    "    else:\n",
    "        l1_corrupted = 0.0\n",
    "        l1_reconstructed = 0.0\n",
    "        improvement = 0.0\n",
    "    \n",
    "    l1_full = F.l1_loss(reconstructed, clean).item()\n",
    "    signal_power = (clean ** 2).mean().item()\n",
    "    noise_power = ((reconstructed - clean) ** 2).mean().item()\n",
    "    snr = 10 * np.log10(signal_power / (noise_power + 1e-8))\n",
    "    \n",
    "    metrics = {\n",
    "        'l1_corrupted_masked': float(l1_corrupted),\n",
    "        'l1_reconstructed_masked': float(l1_reconstructed),\n",
    "        'improvement_pct': float(improvement),\n",
    "        'l1_full': float(l1_full),\n",
    "        'snr_db': float(snr),\n",
    "        'loss_rate': loss_rate,\n",
    "        'packets_lost': int(mask.sum()),\n",
    "        'total_packets': len(mask),\n",
    "        'vocoder': vocoder_type,\n",
    "        'n_mels': 80\n",
    "    }\n",
    "    \n",
    "    with open(output_dir / f\"{base_name}_metrics_{loss_pct}pct{suffix}.json\", 'w') as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(4, 1, figsize=(14, 12))\n",
    "    \n",
    "    axes[0].imshow(clean.numpy().T, aspect='auto', origin='lower', cmap='viridis', vmin=-2, vmax=2)\n",
    "    axes[0].set_title('Clean Audio (80 Mel Bins)', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    axes[1].imshow(corrupted.numpy().T, aspect='auto', origin='lower', cmap='viridis', vmin=-2, vmax=2)\n",
    "    axes[1].set_title(f'Corrupted ({loss_pct}% Packet Loss)', fontsize=14, fontweight='bold')\n",
    "    for i, is_lost in enumerate(mask):\n",
    "        if is_lost:\n",
    "            axes[1].axvline(x=i, color='red', alpha=0.3, linewidth=0.5)\n",
    "    \n",
    "    axes[2].imshow(reconstructed.numpy().T, aspect='auto', origin='lower', cmap='viridis', vmin=-2, vmax=2)\n",
    "    axes[2].set_title(f'Reconstructed (GAN + {vocoder_type.upper()}) - SNR: {snr:.1f} dB', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "    \n",
    "    error = torch.abs(reconstructed - clean)\n",
    "    im = axes[3].imshow(error.numpy().T, aspect='auto', origin='lower', cmap='hot')\n",
    "    axes[3].set_title(f'Error (Improvement: {improvement:.1f}%)', fontsize=14, fontweight='bold')\n",
    "    axes[3].set_xlabel('Time Frames')\n",
    "    plt.colorbar(im, ax=axes[3])\n",
    "    \n",
    "    for ax in axes:\n",
    "        ax.set_ylabel('Mel Bins')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / f\"{base_name}_comparison_{loss_pct}pct{suffix}.png\", \n",
    "               dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"\\n✓ {base_name} @ {loss_pct}% loss\")\n",
    "    print(f\"  Vocoder: {vocoder_type}\")\n",
    "    print(f\"  L1 (masked): {l1_corrupted:.4f} → {l1_reconstructed:.4f}\")\n",
    "    print(f\"  Improvement: {improvement:.1f}%\")\n",
    "    print(f\"  SNR: {snr:.1f} dB\")\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T16:21:21.958149Z",
     "iopub.status.busy": "2025-12-20T16:21:21.957382Z",
     "iopub.status.idle": "2025-12-20T16:21:45.275557Z",
     "shell.execute_reply": "2025-12-20T16:21:45.274783Z",
     "shell.execute_reply.started": "2025-12-20T16:21:21.958120Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading HiFi-GAN vocoder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/NVIDIA/DeepLearningExamples/zipball/torchhub\" to /root/.cache/torch/hub/torchhub.zip\n",
      "Downloading checkpoint from https://api.ngc.nvidia.com/v2/models/nvidia/dle/hifigan__pyt_ckpt_mode-finetune_ds-ljs22khz/versions/21.08.0_amp/files/hifigan_gen_checkpoint_10000_ft.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HiFi-GAN: Removing weight norm.\n",
      "✓ HiFi-GAN loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "vocoder, vocoder_type = load_hifigan(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T16:25:32.423673Z",
     "iopub.status.busy": "2025-12-20T16:25:32.423389Z",
     "iopub.status.idle": "2025-12-20T16:26:07.906056Z",
     "shell.execute_reply": "2025-12-20T16:26:07.905272Z",
     "shell.execute_reply.started": "2025-12-20T16:25:32.423652Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Using HiFi-GAN vocoder (80 mels - perfect match!)...\n",
      "\n",
      "✓ LJ001-0001 @ 20% loss\n",
      "  Vocoder: hifigan\n",
      "  L1 (masked): 0.8275 → 0.6954\n",
      "  Improvement: 16.0%\n",
      "  SNR: 8.2 dB\n",
      "  Using HiFi-GAN vocoder (80 mels - perfect match!)...\n",
      "\n",
      "✓ LJ001-0001 @ 30% loss\n",
      "  Vocoder: hifigan\n",
      "  L1 (masked): 0.8396 → 0.7322\n",
      "  Improvement: 12.8%\n",
      "  SNR: 6.4 dB\n",
      "  Using HiFi-GAN vocoder (80 mels - perfect match!)...\n",
      "\n",
      "✓ LJ001-0001 @ 40% loss\n",
      "  Vocoder: hifigan\n",
      "  L1 (masked): 0.8047 → 0.7156\n",
      "  Improvement: 11.1%\n",
      "  SNR: 5.0 dB\n",
      "  Using HiFi-GAN vocoder (80 mels - perfect match!)...\n",
      "\n",
      "✓ LJ001-0001 @ 50% loss\n",
      "  Vocoder: hifigan\n",
      "  L1 (masked): 0.8318 → 0.7296\n",
      "  Improvement: 12.3%\n",
      "  SNR: 4.0 dB\n",
      "  Using HiFi-GAN vocoder (80 mels - perfect match!)...\n",
      "\n",
      "✓ LJ001-0002 @ 20% loss\n",
      "  Vocoder: hifigan\n",
      "  L1 (masked): 0.7668 → 0.4702\n",
      "  Improvement: 38.7%\n",
      "  SNR: 10.2 dB\n",
      "  Using HiFi-GAN vocoder (80 mels - perfect match!)...\n",
      "\n",
      "✓ LJ001-0002 @ 30% loss\n",
      "  Vocoder: hifigan\n",
      "  L1 (masked): 0.7932 → 0.4912\n",
      "  Improvement: 38.1%\n",
      "  SNR: 8.4 dB\n",
      "  Using HiFi-GAN vocoder (80 mels - perfect match!)...\n",
      "\n",
      "✓ LJ001-0002 @ 40% loss\n",
      "  Vocoder: hifigan\n",
      "  L1 (masked): 0.7851 → 0.4608\n",
      "  Improvement: 41.3%\n",
      "  SNR: 8.4 dB\n",
      "  Using HiFi-GAN vocoder (80 mels - perfect match!)...\n",
      "\n",
      "✓ LJ001-0002 @ 50% loss\n",
      "  Vocoder: hifigan\n",
      "  L1 (masked): 0.8430 → 0.4849\n",
      "  Improvement: 42.5%\n",
      "  SNR: 7.3 dB\n",
      "  Using HiFi-GAN vocoder (80 mels - perfect match!)...\n",
      "\n",
      "✓ LJ001-0003 @ 20% loss\n",
      "  Vocoder: hifigan\n",
      "  L1 (masked): 0.8198 → 0.6694\n",
      "  Improvement: 18.3%\n",
      "  SNR: 8.1 dB\n",
      "  Using HiFi-GAN vocoder (80 mels - perfect match!)...\n",
      "\n",
      "✓ LJ001-0003 @ 30% loss\n",
      "  Vocoder: hifigan\n",
      "  L1 (masked): 0.8055 → 0.6844\n",
      "  Improvement: 15.0%\n",
      "  SNR: 6.0 dB\n",
      "  Using HiFi-GAN vocoder (80 mels - perfect match!)...\n",
      "\n",
      "✓ LJ001-0003 @ 40% loss\n",
      "  Vocoder: hifigan\n",
      "  L1 (masked): 0.7994 → 0.6866\n",
      "  Improvement: 14.1%\n",
      "  SNR: 5.1 dB\n",
      "  Using HiFi-GAN vocoder (80 mels - perfect match!)...\n",
      "\n",
      "✓ LJ001-0003 @ 50% loss\n",
      "  Vocoder: hifigan\n",
      "  L1 (masked): 0.8014 → 0.6970\n",
      "  Improvement: 13.0%\n",
      "  SNR: 3.9 dB\n",
      "  Using HiFi-GAN vocoder (80 mels - perfect match!)...\n",
      "\n",
      "✓ LJ001-0004 @ 20% loss\n",
      "  Vocoder: hifigan\n",
      "  L1 (masked): 0.8206 → 0.6753\n",
      "  Improvement: 17.7%\n",
      "  SNR: 7.9 dB\n",
      "  Using HiFi-GAN vocoder (80 mels - perfect match!)...\n",
      "\n",
      "✓ LJ001-0004 @ 30% loss\n",
      "  Vocoder: hifigan\n",
      "  L1 (masked): 0.8154 → 0.6653\n",
      "  Improvement: 18.4%\n",
      "  SNR: 6.3 dB\n",
      "  Using HiFi-GAN vocoder (80 mels - perfect match!)...\n",
      "\n",
      "✓ LJ001-0004 @ 40% loss\n",
      "  Vocoder: hifigan\n",
      "  L1 (masked): 0.7987 → 0.6776\n",
      "  Improvement: 15.2%\n",
      "  SNR: 5.0 dB\n",
      "  Using HiFi-GAN vocoder (80 mels - perfect match!)...\n",
      "\n",
      "✓ LJ001-0004 @ 50% loss\n",
      "  Vocoder: hifigan\n",
      "  L1 (masked): 0.8327 → 0.6834\n",
      "  Improvement: 17.9%\n",
      "  SNR: 4.2 dB\n",
      "  Using HiFi-GAN vocoder (80 mels - perfect match!)...\n",
      "\n",
      "✓ LJ001-0005 @ 20% loss\n",
      "  Vocoder: hifigan\n",
      "  L1 (masked): 0.7964 → 0.6798\n",
      "  Improvement: 14.6%\n",
      "  SNR: 8.4 dB\n",
      "  Using HiFi-GAN vocoder (80 mels - perfect match!)...\n",
      "\n",
      "✓ LJ001-0005 @ 30% loss\n",
      "  Vocoder: hifigan\n",
      "  L1 (masked): 0.7887 → 0.6604\n",
      "  Improvement: 16.3%\n",
      "  SNR: 6.2 dB\n",
      "  Using HiFi-GAN vocoder (80 mels - perfect match!)...\n",
      "\n",
      "✓ LJ001-0005 @ 40% loss\n",
      "  Vocoder: hifigan\n",
      "  L1 (masked): 0.8085 → 0.6895\n",
      "  Improvement: 14.7%\n",
      "  SNR: 5.0 dB\n",
      "  Using HiFi-GAN vocoder (80 mels - perfect match!)...\n",
      "\n",
      "✓ LJ001-0005 @ 50% loss\n",
      "  Vocoder: hifigan\n",
      "  L1 (masked): 0.8071 → 0.6863\n",
      "  Improvement: 15.0%\n",
      "  SNR: 4.1 dB\n"
     ]
    }
   ],
   "source": [
    "test_files = sorted(list(Path(CONFIG['data_dir']).glob(\"*.wav\")))[:5]\n",
    "test_loss_rates = [0.2, 0.3, 0.4, 0.5]\n",
    "    \n",
    "for audio_file in test_files:\n",
    "    for loss_rate in test_loss_rates:    \n",
    "        reconstruct_and_visualize_80mel(generator, str(audio_file), loss_rate, device, \n",
    "                                               CONFIG['output_dir'], vocoder=vocoder, \n",
    "                                               vocoder_type=vocoder_type\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T14:45:59.400160Z",
     "iopub.status.busy": "2025-12-16T14:45:59.399382Z",
     "iopub.status.idle": "2025-12-16T14:46:00.564975Z",
     "shell.execute_reply": "2025-12-16T14:46:00.564415Z",
     "shell.execute_reply.started": "2025-12-16T14:45:59.400132Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing /kaggle/working/audio_data/LJ001-0001.wav ---\n",
      "Loading SpeechBrain HiFi-GAN...\n",
      "Vocoding...\n",
      "Output Audio Shape: torch.Size([1, 215552])\n",
      "✓ Saved to DEBUG_hifigan_22k_result.wav\n",
      "Please listen to this file specifically.\n",
      "--- Processing /kaggle/working/audio_data/LJ001-0001.wav ---\n",
      "Loading SpeechBrain HiFi-GAN...\n",
      "Vocoding...\n",
      "Output Audio Shape: torch.Size([1, 215552])\n",
      "✓ Saved to DEBUG_hifigan_22k_result.wav\n",
      "Please listen to this file specifically.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def sanity_check_hifigan(audio_path, device='cuda'):\n",
    "    print(f\"--- Processing {audio_path} ---\")\n",
    "    \n",
    "    # 1. Load Audio\n",
    "    waveform, sr = torchaudio.load(audio_path)\n",
    "    if waveform.shape[0] > 1: waveform = waveform.mean(dim=0, keepdim=True)\n",
    "    \n",
    "    # 2. Resample to 22050 (Required for this Vocoder)\n",
    "    resampler = T.Resample(sr, 22050).to(device)\n",
    "    waveform_22k = resampler(waveform.to(device))\n",
    "    \n",
    "    # 3. Generate 80-band Mels (Required for this Vocoder)\n",
    "    mel_transform = T.MelSpectrogram(\n",
    "        sample_rate=22050,\n",
    "        n_fft=1024,\n",
    "        win_length=1024,\n",
    "        hop_length=256,\n",
    "        n_mels=80,\n",
    "        f_min=0.0,\n",
    "        f_max=8000.0,\n",
    "        power=1.0,\n",
    "        normalized=False\n",
    "    ).to(device)\n",
    "    \n",
    "    mel_spec = mel_transform(waveform_22k)\n",
    "    \n",
    "    # 4. Log Transform\n",
    "    log_mel_spec = torch.log(torch.clamp(mel_spec, min=1e-5))\n",
    "    \n",
    "    # 5. Load Vocoder\n",
    "    print(\"Loading SpeechBrain HiFi-GAN...\")\n",
    "    from speechbrain.pretrained import HIFIGAN\n",
    "    vocoder = HIFIGAN.from_hparams(\n",
    "        source=\"speechbrain/tts-hifigan-ljspeech\", \n",
    "        savedir=\"tmpdir_vocoder\",\n",
    "        run_opts={\"device\": str(device)}\n",
    "    )\n",
    "    \n",
    "    # 6. Vocode\n",
    "    print(\"Vocoding...\")\n",
    "    with torch.no_grad():\n",
    "        if log_mel_spec.dim() == 2:\n",
    "            log_mel_spec = log_mel_spec.unsqueeze(0)\n",
    "            \n",
    "        audio_out = vocoder.decode_batch(log_mel_spec)\n",
    "        \n",
    "        # --- FIX FOR RUNTIME ERROR ---\n",
    "        # Ensure audio is strictly (Channels, Time) -> (1, T)\n",
    "        audio_out = audio_out.squeeze() # Remove all extra dims (becomes 1D)\n",
    "        audio_out = audio_out.unsqueeze(0) # Add channel dim (becomes 2D)\n",
    "        \n",
    "        print(f\"Output Audio Shape: {audio_out.shape}\")\n",
    "\n",
    "    # 7. Save\n",
    "    out_path = \"DEBUG_hifigan_22k_result.wav\"\n",
    "    torchaudio.save(out_path, audio_out.cpu(), 22050)\n",
    "    print(f\"✓ Saved to {out_path}\")\n",
    "    print(\"Please listen to this file specifically.\")\n",
    "\n",
    "# Run it\n",
    "if __name__ == \"__main__\":\n",
    "    # Update this path to your file\n",
    "    sanity_check_hifigan(\"/kaggle/working/audio_data/LJ001-0001.wav\")\n",
    "sanity_check_hifigan(\"/kaggle/working/audio_data/LJ001-0001.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T16:26:32.070979Z",
     "iopub.status.busy": "2025-12-20T16:26:32.070275Z",
     "iopub.status.idle": "2025-12-20T16:26:32.242208Z",
     "shell.execute_reply": "2025-12-20T16:26:32.241461Z",
     "shell.execute_reply.started": "2025-12-20T16:26:32.070940Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LJ001-0001_clean_gan.wav\n",
      "LJ001-0001_clean_hifigan_80mel.wav\n",
      "LJ001-0001_comparison_20pct_gan.png\n",
      "LJ001-0001_comparison_20pct_hifigan_80mel.png\n",
      "LJ001-0001_comparison_30pct_gan.png\n",
      "LJ001-0001_comparison_30pct_hifigan_80mel.png\n",
      "LJ001-0001_comparison_40pct_gan.png\n",
      "LJ001-0001_comparison_40pct_hifigan_80mel.png\n",
      "LJ001-0001_comparison_50pct_gan.png\n",
      "LJ001-0001_comparison_50pct_hifigan_80mel.png\n",
      "LJ001-0001_corrupted_20pct_gan.wav\n",
      "LJ001-0001_corrupted_20pct_hifigan_80mel.wav\n",
      "LJ001-0001_corrupted_30pct_gan.wav\n",
      "LJ001-0001_corrupted_30pct_hifigan_80mel.wav\n",
      "LJ001-0001_corrupted_40pct_gan.wav\n",
      "LJ001-0001_corrupted_40pct_hifigan_80mel.wav\n",
      "LJ001-0001_corrupted_50pct_gan.wav\n",
      "LJ001-0001_corrupted_50pct_hifigan_80mel.wav\n",
      "LJ001-0001_metrics_20pct_gan.json\n",
      "LJ001-0001_metrics_20pct_hifigan_80mel.json\n",
      "LJ001-0001_metrics_30pct_gan.json\n",
      "LJ001-0001_metrics_30pct_hifigan_80mel.json\n",
      "LJ001-0001_metrics_40pct_gan.json\n",
      "LJ001-0001_metrics_40pct_hifigan_80mel.json\n",
      "LJ001-0001_metrics_50pct_gan.json\n",
      "LJ001-0001_metrics_50pct_hifigan_80mel.json\n",
      "LJ001-0001_reconstructed_20pct_gan.wav\n",
      "LJ001-0001_reconstructed_20pct_hifigan_80mel.wav\n",
      "LJ001-0001_reconstructed_30pct_gan.wav\n",
      "LJ001-0001_reconstructed_30pct_hifigan_80mel.wav\n",
      "LJ001-0001_reconstructed_40pct_gan.wav\n",
      "LJ001-0001_reconstructed_40pct_hifigan_80mel.wav\n",
      "LJ001-0001_reconstructed_50pct_gan.wav\n",
      "LJ001-0001_reconstructed_50pct_hifigan_80mel.wav\n",
      "LJ001-0002_clean_gan.wav\n",
      "LJ001-0002_clean_hifigan_80mel.wav\n",
      "LJ001-0002_comparison_20pct_gan.png\n",
      "LJ001-0002_comparison_20pct_hifigan_80mel.png\n",
      "LJ001-0002_comparison_30pct_gan.png\n",
      "LJ001-0002_comparison_30pct_hifigan_80mel.png\n",
      "LJ001-0002_comparison_40pct_gan.png\n",
      "LJ001-0002_comparison_40pct_hifigan_80mel.png\n",
      "LJ001-0002_comparison_50pct_gan.png\n",
      "LJ001-0002_comparison_50pct_hifigan_80mel.png\n",
      "LJ001-0002_corrupted_20pct_gan.wav\n",
      "LJ001-0002_corrupted_20pct_hifigan_80mel.wav\n",
      "LJ001-0002_corrupted_30pct_gan.wav\n",
      "LJ001-0002_corrupted_30pct_hifigan_80mel.wav\n",
      "LJ001-0002_corrupted_40pct_gan.wav\n",
      "LJ001-0002_corrupted_40pct_hifigan_80mel.wav\n",
      "LJ001-0002_corrupted_50pct_gan.wav\n",
      "LJ001-0002_corrupted_50pct_hifigan_80mel.wav\n",
      "LJ001-0002_metrics_20pct_gan.json\n",
      "LJ001-0002_metrics_20pct_hifigan_80mel.json\n",
      "LJ001-0002_metrics_30pct_gan.json\n",
      "LJ001-0002_metrics_30pct_hifigan_80mel.json\n",
      "LJ001-0002_metrics_40pct_gan.json\n",
      "LJ001-0002_metrics_40pct_hifigan_80mel.json\n",
      "LJ001-0002_metrics_50pct_gan.json\n",
      "LJ001-0002_metrics_50pct_hifigan_80mel.json\n",
      "LJ001-0002_reconstructed_20pct_gan.wav\n",
      "LJ001-0002_reconstructed_20pct_hifigan_80mel.wav\n",
      "LJ001-0002_reconstructed_30pct_gan.wav\n",
      "LJ001-0002_reconstructed_30pct_hifigan_80mel.wav\n",
      "LJ001-0002_reconstructed_40pct_gan.wav\n",
      "LJ001-0002_reconstructed_40pct_hifigan_80mel.wav\n",
      "LJ001-0002_reconstructed_50pct_gan.wav\n",
      "LJ001-0002_reconstructed_50pct_hifigan_80mel.wav\n",
      "LJ001-0003_clean_gan.wav\n",
      "LJ001-0003_clean_hifigan_80mel.wav\n",
      "LJ001-0003_comparison_20pct_gan.png\n",
      "LJ001-0003_comparison_20pct_hifigan_80mel.png\n",
      "LJ001-0003_comparison_30pct_gan.png\n",
      "LJ001-0003_comparison_30pct_hifigan_80mel.png\n",
      "LJ001-0003_comparison_40pct_gan.png\n",
      "LJ001-0003_comparison_40pct_hifigan_80mel.png\n",
      "LJ001-0003_comparison_50pct_gan.png\n",
      "LJ001-0003_comparison_50pct_hifigan_80mel.png\n",
      "LJ001-0003_corrupted_20pct_gan.wav\n",
      "LJ001-0003_corrupted_20pct_hifigan_80mel.wav\n",
      "LJ001-0003_corrupted_30pct_gan.wav\n",
      "LJ001-0003_corrupted_30pct_hifigan_80mel.wav\n",
      "LJ001-0003_corrupted_40pct_gan.wav\n",
      "LJ001-0003_corrupted_40pct_hifigan_80mel.wav\n",
      "LJ001-0003_corrupted_50pct_gan.wav\n",
      "LJ001-0003_corrupted_50pct_hifigan_80mel.wav\n",
      "LJ001-0003_metrics_20pct_gan.json\n",
      "LJ001-0003_metrics_20pct_hifigan_80mel.json\n",
      "LJ001-0003_metrics_30pct_gan.json\n",
      "LJ001-0003_metrics_30pct_hifigan_80mel.json\n",
      "LJ001-0003_metrics_40pct_gan.json\n",
      "LJ001-0003_metrics_40pct_hifigan_80mel.json\n",
      "LJ001-0003_metrics_50pct_gan.json\n",
      "LJ001-0003_metrics_50pct_hifigan_80mel.json\n",
      "LJ001-0003_reconstructed_20pct_gan.wav\n",
      "LJ001-0003_reconstructed_20pct_hifigan_80mel.wav\n",
      "LJ001-0003_reconstructed_30pct_gan.wav\n",
      "LJ001-0003_reconstructed_30pct_hifigan_80mel.wav\n",
      "LJ001-0003_reconstructed_40pct_gan.wav\n",
      "LJ001-0003_reconstructed_40pct_hifigan_80mel.wav\n",
      "LJ001-0003_reconstructed_50pct_gan.wav\n",
      "LJ001-0003_reconstructed_50pct_hifigan_80mel.wav\n",
      "LJ001-0004_clean_gan.wav\n",
      "LJ001-0004_clean_hifigan_80mel.wav\n",
      "LJ001-0004_comparison_20pct_gan.png\n",
      "LJ001-0004_comparison_20pct_hifigan_80mel.png\n",
      "LJ001-0004_comparison_30pct_gan.png\n",
      "LJ001-0004_comparison_30pct_hifigan_80mel.png\n",
      "LJ001-0004_comparison_40pct_gan.png\n",
      "LJ001-0004_comparison_40pct_hifigan_80mel.png\n",
      "LJ001-0004_comparison_50pct_gan.png\n",
      "LJ001-0004_comparison_50pct_hifigan_80mel.png\n",
      "LJ001-0004_corrupted_20pct_gan.wav\n",
      "LJ001-0004_corrupted_20pct_hifigan_80mel.wav\n",
      "LJ001-0004_corrupted_30pct_gan.wav\n",
      "LJ001-0004_corrupted_30pct_hifigan_80mel.wav\n",
      "LJ001-0004_corrupted_40pct_gan.wav\n",
      "LJ001-0004_corrupted_40pct_hifigan_80mel.wav\n",
      "LJ001-0004_corrupted_50pct_gan.wav\n",
      "LJ001-0004_corrupted_50pct_hifigan_80mel.wav\n",
      "LJ001-0004_metrics_20pct_gan.json\n",
      "LJ001-0004_metrics_20pct_hifigan_80mel.json\n",
      "LJ001-0004_metrics_30pct_gan.json\n",
      "LJ001-0004_metrics_30pct_hifigan_80mel.json\n",
      "LJ001-0004_metrics_40pct_gan.json\n",
      "LJ001-0004_metrics_40pct_hifigan_80mel.json\n",
      "LJ001-0004_metrics_50pct_gan.json\n",
      "LJ001-0004_metrics_50pct_hifigan_80mel.json\n",
      "LJ001-0004_reconstructed_20pct_gan.wav\n",
      "LJ001-0004_reconstructed_20pct_hifigan_80mel.wav\n",
      "LJ001-0004_reconstructed_30pct_gan.wav\n",
      "LJ001-0004_reconstructed_30pct_hifigan_80mel.wav\n",
      "LJ001-0004_reconstructed_40pct_gan.wav\n",
      "LJ001-0004_reconstructed_40pct_hifigan_80mel.wav\n",
      "LJ001-0004_reconstructed_50pct_gan.wav\n",
      "LJ001-0004_reconstructed_50pct_hifigan_80mel.wav\n",
      "LJ001-0005_clean_gan.wav\n",
      "LJ001-0005_clean_hifigan_80mel.wav\n",
      "LJ001-0005_comparison_20pct_gan.png\n",
      "LJ001-0005_comparison_20pct_hifigan_80mel.png\n",
      "LJ001-0005_comparison_30pct_gan.png\n",
      "LJ001-0005_comparison_30pct_hifigan_80mel.png\n",
      "LJ001-0005_comparison_40pct_gan.png\n",
      "LJ001-0005_comparison_40pct_hifigan_80mel.png\n",
      "LJ001-0005_comparison_50pct_gan.png\n",
      "LJ001-0005_comparison_50pct_hifigan_80mel.png\n",
      "LJ001-0005_corrupted_20pct_gan.wav\n",
      "LJ001-0005_corrupted_20pct_hifigan_80mel.wav\n",
      "LJ001-0005_corrupted_30pct_gan.wav\n",
      "LJ001-0005_corrupted_30pct_hifigan_80mel.wav\n",
      "LJ001-0005_corrupted_40pct_gan.wav\n",
      "LJ001-0005_corrupted_40pct_hifigan_80mel.wav\n",
      "LJ001-0005_corrupted_50pct_gan.wav\n",
      "LJ001-0005_corrupted_50pct_hifigan_80mel.wav\n",
      "LJ001-0005_metrics_20pct_gan.json\n",
      "LJ001-0005_metrics_20pct_hifigan_80mel.json\n",
      "LJ001-0005_metrics_30pct_gan.json\n",
      "LJ001-0005_metrics_30pct_hifigan_80mel.json\n",
      "LJ001-0005_metrics_40pct_gan.json\n",
      "LJ001-0005_metrics_40pct_hifigan_80mel.json\n",
      "LJ001-0005_metrics_50pct_gan.json\n",
      "LJ001-0005_metrics_50pct_hifigan_80mel.json\n",
      "LJ001-0005_reconstructed_20pct_gan.wav\n",
      "LJ001-0005_reconstructed_20pct_hifigan_80mel.wav\n",
      "LJ001-0005_reconstructed_30pct_gan.wav\n",
      "LJ001-0005_reconstructed_30pct_hifigan_80mel.wav\n",
      "LJ001-0005_reconstructed_40pct_gan.wav\n",
      "LJ001-0005_reconstructed_40pct_hifigan_80mel.wav\n",
      "LJ001-0005_reconstructed_50pct_gan.wav\n",
      "LJ001-0005_reconstructed_50pct_hifigan_80mel.wav\n"
     ]
    }
   ],
   "source": [
    "!ls /kaggle/working/outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(filename=\"/kaggle/working/outputs/LJ001-0001_clean_gan.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(filename=\"/kaggle/working/outputs/LJ001-0001_corrupted_40pct_gan.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(filename=\"/kaggle/working/outputs/LJ001-0001_reconstructed_40pct_gan.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T14:58:38.802006Z",
     "iopub.status.busy": "2025-12-16T14:58:38.801751Z",
     "iopub.status.idle": "2025-12-16T14:58:38.965431Z",
     "shell.execute_reply": "2025-12-16T14:58:38.964562Z",
     "shell.execute_reply.started": "2025-12-16T14:58:38.801987Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LJ001-0001_clean_gan.wav\n",
      "LJ001-0001_clean_hifigan.wav\n",
      "LJ001-0001_comparison_20pct_gan.png\n",
      "LJ001-0001_comparison_30pct_gan.png\n",
      "LJ001-0001_comparison_40pct_gan.png\n",
      "LJ001-0001_comparison_50pct_gan.png\n",
      "LJ001-0001_corrupted_20pct_gan.wav\n",
      "LJ001-0001_corrupted_20pct_hifigan.wav\n",
      "LJ001-0001_corrupted_30pct_gan.wav\n",
      "LJ001-0001_corrupted_30pct_hifigan.wav\n",
      "LJ001-0001_corrupted_40pct_gan.wav\n",
      "LJ001-0001_corrupted_40pct_hifigan.wav\n",
      "LJ001-0001_corrupted_50pct_gan.wav\n",
      "LJ001-0001_corrupted_50pct_hifigan.wav\n",
      "LJ001-0001_metrics_20pct_gan.json\n",
      "LJ001-0001_metrics_20pct_hifigan.json\n",
      "LJ001-0001_metrics_30pct_gan.json\n",
      "LJ001-0001_metrics_30pct_hifigan.json\n",
      "LJ001-0001_metrics_40pct_gan.json\n",
      "LJ001-0001_metrics_40pct_hifigan.json\n",
      "LJ001-0001_metrics_50pct_gan.json\n",
      "LJ001-0001_metrics_50pct_hifigan.json\n",
      "LJ001-0001_reconstructed_20pct_gan.wav\n",
      "LJ001-0001_reconstructed_20pct_hifigan.wav\n",
      "LJ001-0001_reconstructed_30pct_gan.wav\n",
      "LJ001-0001_reconstructed_30pct_hifigan.wav\n",
      "LJ001-0001_reconstructed_40pct_gan.wav\n",
      "LJ001-0001_reconstructed_40pct_hifigan.wav\n",
      "LJ001-0001_reconstructed_50pct_gan.wav\n",
      "LJ001-0001_reconstructed_50pct_hifigan.wav\n",
      "LJ001-0002_clean_gan.wav\n",
      "LJ001-0002_clean_hifigan.wav\n",
      "LJ001-0002_comparison_20pct_gan.png\n",
      "LJ001-0002_comparison_30pct_gan.png\n",
      "LJ001-0002_comparison_40pct_gan.png\n",
      "LJ001-0002_comparison_50pct_gan.png\n",
      "LJ001-0002_corrupted_20pct_gan.wav\n",
      "LJ001-0002_corrupted_20pct_hifigan.wav\n",
      "LJ001-0002_corrupted_30pct_gan.wav\n",
      "LJ001-0002_corrupted_30pct_hifigan.wav\n",
      "LJ001-0002_corrupted_40pct_gan.wav\n",
      "LJ001-0002_corrupted_40pct_hifigan.wav\n",
      "LJ001-0002_corrupted_50pct_gan.wav\n",
      "LJ001-0002_corrupted_50pct_hifigan.wav\n",
      "LJ001-0002_metrics_20pct_gan.json\n",
      "LJ001-0002_metrics_20pct_hifigan.json\n",
      "LJ001-0002_metrics_30pct_gan.json\n",
      "LJ001-0002_metrics_30pct_hifigan.json\n",
      "LJ001-0002_metrics_40pct_gan.json\n",
      "LJ001-0002_metrics_40pct_hifigan.json\n",
      "LJ001-0002_metrics_50pct_gan.json\n",
      "LJ001-0002_metrics_50pct_hifigan.json\n",
      "LJ001-0002_reconstructed_20pct_gan.wav\n",
      "LJ001-0002_reconstructed_20pct_hifigan.wav\n",
      "LJ001-0002_reconstructed_30pct_gan.wav\n",
      "LJ001-0002_reconstructed_30pct_hifigan.wav\n",
      "LJ001-0002_reconstructed_40pct_gan.wav\n",
      "LJ001-0002_reconstructed_40pct_hifigan.wav\n",
      "LJ001-0002_reconstructed_50pct_gan.wav\n",
      "LJ001-0002_reconstructed_50pct_hifigan.wav\n",
      "LJ001-0003_clean_gan.wav\n",
      "LJ001-0003_clean_hifigan.wav\n",
      "LJ001-0003_comparison_20pct_gan.png\n",
      "LJ001-0003_comparison_30pct_gan.png\n",
      "LJ001-0003_comparison_40pct_gan.png\n",
      "LJ001-0003_comparison_50pct_gan.png\n",
      "LJ001-0003_corrupted_20pct_gan.wav\n",
      "LJ001-0003_corrupted_20pct_hifigan.wav\n",
      "LJ001-0003_corrupted_30pct_gan.wav\n",
      "LJ001-0003_corrupted_30pct_hifigan.wav\n",
      "LJ001-0003_corrupted_40pct_gan.wav\n",
      "LJ001-0003_corrupted_40pct_hifigan.wav\n",
      "LJ001-0003_corrupted_50pct_gan.wav\n",
      "LJ001-0003_corrupted_50pct_hifigan.wav\n",
      "LJ001-0003_metrics_20pct_gan.json\n",
      "LJ001-0003_metrics_20pct_hifigan.json\n",
      "LJ001-0003_metrics_30pct_gan.json\n",
      "LJ001-0003_metrics_30pct_hifigan.json\n",
      "LJ001-0003_metrics_40pct_gan.json\n",
      "LJ001-0003_metrics_40pct_hifigan.json\n",
      "LJ001-0003_metrics_50pct_gan.json\n",
      "LJ001-0003_metrics_50pct_hifigan.json\n",
      "LJ001-0003_reconstructed_20pct_gan.wav\n",
      "LJ001-0003_reconstructed_20pct_hifigan.wav\n",
      "LJ001-0003_reconstructed_30pct_gan.wav\n",
      "LJ001-0003_reconstructed_30pct_hifigan.wav\n",
      "LJ001-0003_reconstructed_40pct_gan.wav\n",
      "LJ001-0003_reconstructed_40pct_hifigan.wav\n",
      "LJ001-0003_reconstructed_50pct_gan.wav\n",
      "LJ001-0003_reconstructed_50pct_hifigan.wav\n",
      "LJ001-0004_clean_gan.wav\n",
      "LJ001-0004_clean_hifigan.wav\n",
      "LJ001-0004_comparison_20pct_gan.png\n",
      "LJ001-0004_comparison_30pct_gan.png\n",
      "LJ001-0004_comparison_40pct_gan.png\n",
      "LJ001-0004_comparison_50pct_gan.png\n",
      "LJ001-0004_corrupted_20pct_gan.wav\n",
      "LJ001-0004_corrupted_20pct_hifigan.wav\n",
      "LJ001-0004_corrupted_30pct_gan.wav\n",
      "LJ001-0004_corrupted_30pct_hifigan.wav\n",
      "LJ001-0004_corrupted_40pct_gan.wav\n",
      "LJ001-0004_corrupted_40pct_hifigan.wav\n",
      "LJ001-0004_corrupted_50pct_gan.wav\n",
      "LJ001-0004_corrupted_50pct_hifigan.wav\n",
      "LJ001-0004_metrics_20pct_gan.json\n",
      "LJ001-0004_metrics_20pct_hifigan.json\n",
      "LJ001-0004_metrics_30pct_gan.json\n",
      "LJ001-0004_metrics_30pct_hifigan.json\n",
      "LJ001-0004_metrics_40pct_gan.json\n",
      "LJ001-0004_metrics_40pct_hifigan.json\n",
      "LJ001-0004_metrics_50pct_gan.json\n",
      "LJ001-0004_metrics_50pct_hifigan.json\n",
      "LJ001-0004_reconstructed_20pct_gan.wav\n",
      "LJ001-0004_reconstructed_20pct_hifigan.wav\n",
      "LJ001-0004_reconstructed_30pct_gan.wav\n",
      "LJ001-0004_reconstructed_30pct_hifigan.wav\n",
      "LJ001-0004_reconstructed_40pct_gan.wav\n",
      "LJ001-0004_reconstructed_40pct_hifigan.wav\n",
      "LJ001-0004_reconstructed_50pct_gan.wav\n",
      "LJ001-0004_reconstructed_50pct_hifigan.wav\n",
      "LJ001-0005_clean_gan.wav\n",
      "LJ001-0005_clean_hifigan.wav\n",
      "LJ001-0005_comparison_20pct_gan.png\n",
      "LJ001-0005_comparison_30pct_gan.png\n",
      "LJ001-0005_comparison_40pct_gan.png\n",
      "LJ001-0005_comparison_50pct_gan.png\n",
      "LJ001-0005_corrupted_20pct_gan.wav\n",
      "LJ001-0005_corrupted_20pct_hifigan.wav\n",
      "LJ001-0005_corrupted_30pct_gan.wav\n",
      "LJ001-0005_corrupted_30pct_hifigan.wav\n",
      "LJ001-0005_corrupted_40pct_gan.wav\n",
      "LJ001-0005_corrupted_40pct_hifigan.wav\n",
      "LJ001-0005_corrupted_50pct_gan.wav\n",
      "LJ001-0005_corrupted_50pct_hifigan.wav\n",
      "LJ001-0005_metrics_20pct_gan.json\n",
      "LJ001-0005_metrics_20pct_hifigan.json\n",
      "LJ001-0005_metrics_30pct_gan.json\n",
      "LJ001-0005_metrics_30pct_hifigan.json\n",
      "LJ001-0005_metrics_40pct_gan.json\n",
      "LJ001-0005_metrics_40pct_hifigan.json\n",
      "LJ001-0005_metrics_50pct_gan.json\n",
      "LJ001-0005_metrics_50pct_hifigan.json\n",
      "LJ001-0005_reconstructed_20pct_gan.wav\n",
      "LJ001-0005_reconstructed_20pct_hifigan.wav\n",
      "LJ001-0005_reconstructed_30pct_gan.wav\n",
      "LJ001-0005_reconstructed_30pct_hifigan.wav\n",
      "LJ001-0005_reconstructed_40pct_gan.wav\n",
      "LJ001-0005_reconstructed_40pct_hifigan.wav\n",
      "LJ001-0005_reconstructed_50pct_gan.wav\n",
      "LJ001-0005_reconstructed_50pct_hifigan.wav\n"
     ]
    }
   ],
   "source": [
    "!ls /kaggle/working/outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T13:48:38.702278Z",
     "iopub.status.busy": "2025-12-16T13:48:38.701705Z",
     "iopub.status.idle": "2025-12-16T13:48:38.855827Z",
     "shell.execute_reply": "2025-12-16T13:48:38.855061Z",
     "shell.execute_reply.started": "2025-12-16T13:48:38.702245Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_gan.pt  training_curves_gan.png  training_history_gan.json\n"
     ]
    }
   ],
   "source": [
    "!ls /kaggle/working/checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T15:43:37.034461Z",
     "iopub.status.busy": "2025-12-20T15:43:37.034128Z",
     "iopub.status.idle": "2025-12-20T15:43:40.174743Z",
     "shell.execute_reply": "2025-12-20T15:43:40.173897Z",
     "shell.execute_reply.started": "2025-12-20T15:43:37.034437Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: /kaggle/working/checkpoints.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "folder_path = \"/kaggle/working/checkpoints\"\n",
    "output_path = \"/kaggle/working/checkpoints.tar.gz\"\n",
    "\n",
    "with tarfile.open(output_path, \"w:gz\") as tar:\n",
    "    tar.add(folder_path, arcname=\"my_folder\")\n",
    "\n",
    "print(\"Created:\", output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corrupted 40%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<audio controls src=\"LJ001-0001_corrupted_40pct_gan (1).wav\" title=\"Title\"></audio>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconstructed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<audio controls src=\"LJ001-0001_reconstructed_40pct_gan (1).wav\" title=\"Title\"></audio>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<audio controls src=\"LJ001-0001_clean.wav\" title=\"Title\"></audio>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The warbly, bubbly sound is because of Griffin Lim. Need a better vocoder the HiFi GAN does not work, either need to tweak settings or change the vocoder. With griffin lim sounds fine but the warbly-ness caused by phase shifting (that is fixed with a vocoder).\n",
    "Some choppiness exists in reconstructed but can be fixed with longer training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "isSourceIdPinned": true,
     "modelId": 535206,
     "modelInstanceId": 520975,
     "sourceId": 688557,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
